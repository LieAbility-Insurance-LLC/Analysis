# Motor Insurance **Fraudâ€‘Detection** Platform

> Endâ€‘toâ€‘end machineâ€‘learning pipeline + Streamlit dashboard for classifying fraudulent motorâ€‘insurance claims with both **supervised** and **unsupervised** techniques.

---

## TableÂ ofÂ Contents

1. [Project Overview](#project-overview)
2. [Key Features](#key-features)
3. [QuickÂ Start](#quick-start)
4. [Repository Layout](#repository-layout)
5. [UsageÂ Examples](#usage-examples)
6. [Streamlit Dashboard](#streamlit-dashboard)
7. [Extensibility](#extensibility)
8. [License](#license)

---

## ProjectÂ Overview

Motorâ€‘insurance fraud inflates premiums worldâ€‘wide and can cost carriers billions ofÂ USD annually.  This project provides an **academicâ€‘grade yet productionâ€‘ready** codeâ€‘base that:

* Cleans & engineers claimâ€‘level data (â‰ˆÂ 40 predictors)
* Trains multiple models (LogisticÂ RegressionÂ â†’Â BalancedÂ RandomÂ ForestÂ â†’Â IsolationÂ Forest)
* Benchmarks them via ROCâ€‘AUC, PRâ€‘AUC, lift, cumulativeâ€‘gain & reliability curves
* Explains predictions with SHAP
* Serves an interactive **Streamlit** dashboard for EDA, training and prediction (batch & singleâ€‘claim)

Although the reference dataset is an anonymised CSV (`insurance_claims.csv`), the pipeline generalises to any tabular claims file that contains a binary `fraud_reported` target.

---

## KeyÂ Features

| Category             | Highlights                                                                                                                                                                                         |
| -------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Data Engineering** | â€¢ Robust missingâ€‘value handling & outlier capping  <br>â€¢ Domainâ€‘aware features (e.g. `is_rush_hour`)                                                                                               |
| **Model Zoo**        | â€¢ Classic supervised learners *(LR, DT, RF, GB, XGB, SVM)*  <br>â€¢ Ensemble stacking + isotonic calibration  <br>â€¢ Unsupervised detectors *(IsolationÂ Forest, LOF, Oneâ€‘ClassÂ SVM, Kâ€‘Means, DBSCAN)* |
| **Autoâ€‘Tuning**      | Gridâ€‘search for RandomÂ Forest hyperâ€‘parameters with crossâ€‘validated F1 objective                                                                                                                   |
| **Evaluation Suite** | Confusion heatâ€‘maps, ROC, PR, lift, cumulativeâ€‘gain, calibration, probability densities                                                                                                            |
| **Explainability**   | Global & local SHAP (bar + force plots)                                                                                                                                                            |
| **VisualÂ EDA**       | Class balance, correlation heatâ€‘map, violin plots, tâ€‘SNE projection                                                                                                                                |
| **Dashboard**        | Zeroâ€‘code UI built with StreamlitÂ â†’ upload data, train, explain, predict, download results                                                                                                         |
| **Modularity**       | Each stage lives in its own module (`preprocessing.py`, `evaluation.py`, â€¦) for easy reuse                                                                                                         |

---

## QuickÂ Start

### 1Â Â·Â CloneÂ &Â Setup

```bash
# clone the repo
$ git clone https://github.com/LieAbility-Insurance-LLC/Analysis.git
$ cd insuranceâ€‘fraudâ€‘ml

# create virtual env (recommended)
$ python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate

# install dependencies in one go
$ python setup_env.py        # installs pandas, scikitâ€‘learn, imbalancedâ€‘learn, xgboost, shap, â€¦
```

### 2Â Â·Â Prepare Data

Place `insurance_claims.csv` in the project root **or** point to your own CSV with identical column names (target columnÂ = `fraud_reported`, values `Y`/`N` or `1`/`0`).

### 3Â Â·Â Commandâ€‘LineÂ Pipeline

```bash
# trains models, saves plots â†’ ./figures, best model â†’ ./models
$ python main.py
```

Logs appear in the terminal; plots are written as `fig_000.png`, `fig_001.png`, â€¦

### 4Â Â·Â LaunchÂ the Dashboard

```bash
$ streamlit run app.py
```

Open [http://localhost:8501](http://localhost:8501) in your browser and exploreÂ ğŸ“Š

---

## RepositoryÂ Layout

```
.
â”œâ”€â”€ app.py                # Streamlit dashboard (UI layer)
â”œâ”€â”€ main.py               # Endâ€‘toâ€‘end CLI workflow
â”œâ”€â”€ setup_env.py          # Oneâ€‘shot dependency installer
â”œâ”€â”€ data_handling.py      # Safe loading + schema validation
â”œâ”€â”€ preprocessing.py      # Cleaning, encoding, scaling
â”œâ”€â”€ feature_engineering.py# Domainâ€‘specific feature additions + topâ€‘N selector
â”œâ”€â”€ model_training.py     # Supervised & unsupervised model routines
â”œâ”€â”€ evaluation.py         # Metrics, plots, SHAP helpers
â”œâ”€â”€ models/               # â† autoâ€‘saved .pkl models
â”œâ”€â”€ figures/              # â† autoâ€‘generated PNG plots
â””â”€â”€ README.md             # You are here ğŸ˜‰
```

---

## UsageÂ Examples

### BatchÂ Scoring with Saved Model

```python
import joblib, pandas as pd
from preprocessing import preprocess_data
from feature_engineering import feature_engineering

# 1Â Â·Â Load artefacts
model  = joblib.load("models/best_model.pkl")
preproc= joblib.load("models/preprocessor.pkl")

# 2Â Â·Â Prepare new claims
new = pd.read_csv("new_claims.csv")
X = feature_engineering(preprocess_data(new))
X_enc = preproc.transform(X.drop(columns=["fraud_reported"], errors="ignore"))

# 3Â Â·Â Predict probability of fraud
proba = model.predict_proba(X_enc)[:, 1]
print(pd.Series(proba, name="fraud_probability"))
```

### ExtendingÂ with Additional Models

Add any scikitâ€‘learn compatible estimator in **`model_training.py â†’ models_sup`** and it will automatically inherit evaluation plots with no extra code.

---

## StreamlitÂ Dashboard

<p align="center">
  <img width="740" alt="dashboard" src="https://github.com/yourâ€‘org/insuranceâ€‘fraudâ€‘ml/raw/main/docs/dashboard_light.png">
</p>

* **Upload** or autoâ€‘load sample data
* **EDA**: class balance, correlation, tâ€‘SNE
* **Preprocess**: oneâ€‘click cleaning + feature engineering
* **Train**: optional hyperâ€‘parameter tuning, SMOTE balancing
* **Predict**: batch CSV or singleâ€‘claim form
* **Explain**: SHAP bar / force plots rendered inline

---

## Extensibility

| Task                            | Where to HookÂ In                                                                                            |
| ------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| Replace SMOTE with ADASYN       | `main.py â†’ SMOTE` line Â· change sampler                                                                     |
| Log to MLflow / Weights\&Biases | Wrap training loop in `main.py` (or `app.py`) with your tracking calls                                      |
| Serve as REST API               | Save model & `preprocessor.pkl` then expose via FastAPI (`joblib.load` at startup)                          |
| New plots                       | Add matplotlib/Seaborn functions in `evaluation.py` â€“ they autoâ€‘save via the global `plt.show` monkeyâ€‘patch |

---

## License

Distributed under the **MIT License** â€“ see [`LICENSE`](LICENSE) for details.
